<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Getting started with quantum field theory</title>
  <meta name="description" content="A Learner's Guide to Advanced Theoretical Physics">

  <!-- Uncomment once we actually have a favicon
  
      <link rel="icon" type="image/svg+xml" href="https://www.learntheoreticalphysics.com/favicon.svg">
      <link rel="icon" type="image/png" href="https://www.learntheoreticalphysics.com/favicon.ico">
  
  -->

  
      <!--KaTeX-->
      <link rel="stylesheet" href="https://www.learntheoreticalphysics.com/katex/katex.min.css">

      <link rel="stylesheet" href="https://www.learntheoreticalphysics.com/site.css">
  

</head>

<body>
  
    <article class="post">
        <h1>Getting started with quantum field theory</h2>
        
        <p id="print-notice">This page is print-friendly. Simply press Ctrl + P (or Command + P if you use a Mac) to print the page and download it as a PDF.</p>
        
        
        <!-- table of contents -->
        
        <details class="toc" id="toc" open>
        <summary>Table of contents</summary>
        <p class="toc-info">Note: it is highly recommended to navigate by clicking links in the table of contents! It means you can use the back button in your browser to go back to any section you were reading, so you can jump back and forth between sections!</p>
        <div>
            <ul>
            
                <li>
                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#the-motivation-for-quantum-field-theory">The motivation for quantum field theory</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#things-to-know">Things to know</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#an-overview-of-classical-field-theory">An overview of classical field theory</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#the-principle-of-stationary-action">The Principle of Stationary Action</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#an-overview-of-tensors">An overview of tensors</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#what-is-a-tensor">What is a tensor?</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#tensor-algebra-and-calculus">Tensor algebra and calculus</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#upper-and-lower-indices">Upper and lower indices</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#free-indices-dummy-indices-and-the-einstein-summation-convention">Free indices, dummy indices, and the Einstein summation convention</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#relabeling-indices-with-the-kronecker-delta">Relabeling indices with the Kronecker delta</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#raising-and-lowering-indices">Raising and lowering indices</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#tensor-contraction">Tensor contraction</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#practicing-contractions">Practicing contractions</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#tensor-calculus">Tensor calculus</a>
                                </li>
                            
                                <li>
                                    <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/chapter-1/#relativity-and-spacetime">Relativity and spacetime</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
            </ul>
        </div>
        </details>
        

        <!-- page content -->
        <h2 id="the-motivation-for-quantum-field-theory">The motivation for quantum field theory</h2>
<p>As far as we know, all matter in the universe is quantum in nature. Quantum mechanics governs the behavior of all matter, and at an introductory and intermediate level, the dynamics of quantum systems is typically solved with the Schrödinger equation:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t} \Psi(\mathbf{r}, t) = \left(-\dfrac{\hbar^2}{2m} \nabla^2 + V(\mathbf{r})\right) \Psi(\mathbf{r}, t)
$$
</p>
<p>However, relativistic quantum field theory is a far more accurate theory of quantum mechanics than the Schrödinger equation. In fact, while we may use approximations like (semi-)classical theory and nonrelativistic or single-particle quantum mechanics, quantum field theory, and specifically the Standard Model, offers the best and most accurate results. </p>
<p>That is to say, any quantum system may be solved for by the Schrödinger equation, but doing the same calculations with quantum field theory offers results with unparalleled precision. Predictions of the <a href="https://en.wikipedia.org/wiki/Magnetic_moment">magnetic moment of electrons</a>, <a href="https://en.wikipedia.org/wiki/Lamb_shift">energy levels of the hydrogen atom</a>, a variety of <a href="https://physics.info/standard/">previously-unknown elementary particles</a>, and the <a href="https://en.wikipedia.org/wiki/Rydberg_constant">Rydberg constant</a> made through quantum field theory have been experimentally tested and confirmed very well.</p>
<p>These are not meaningless predictions either; our understanding of the emission and absorption of light, of the subatomic origins of magnetism, and even our standard of time in the <a href="https://en.wikipedia.org/wiki/Second">definition of a second</a> are based on these predictions. In turn, numerous scientific experiments, material science, laser technology, and electronics depend on applying these predictions, without which they would likely not progress to how they are today.</p>
<h3 id="things-to-know">Things to know</h3>
<p>We'll be working with natural units where $c = \hbar = 1$ simply out of convenience. Also, (almost) everything will be in tensors. We will be working in the formalism of Lagrangian and Hamiltonian field theory, as is standard in theoretical physics. Don't worry if some of these topics are unfamiliar; we will review all of these topics before we begin, and consult the other free books on this site for more information.</p>
<h2 id="an-overview-of-classical-field-theory">An overview of classical field theory</h2>
<p>Before we get started with the complex quantum
field theories of the Standard Model, we must look at their shared
fundamental structure in <em>classical field theory</em>. A classical field
theory describes a <em>field</em>, which can be thought of as a physical
quantity that fills all of space and allows interactions between
particles to propagate between each other, leading to what we perceive
as &quot;force&quot;.</p>
<p>The nebulous idea of a field can be more easily quantified by
considering the following example: you stand in a room that has varying
temperature. The temperature changes depending on where you are in the
room (a physicist would call that <em>position dependence</em>), and the
temperature also changes over time (in the language of physics, <em>time
dependence</em>). Thus, if we let $\vec x = \langle x, y, z\rangle$ denote
the position in the room and $t$ denote the time, we can write the
temperature field, represented as $\phi$, with:</p>
<p class="mathcell">
$$
\phi(\vec x, t)
$$
</p>
<p>Our modern formulation of classical field theory is that of the
<strong>Lagrangian and Hamiltonian formulation</strong> of classical fields. Such a
formulation begins by describing a field in terms of a specific function
of the field, called a <strong>Lagrangian</strong> , and written as
$\mathscr{L}(\phi, \vec x, t)$, where, again, $\phi$ is a function of
$\vec x$ and $t$, that is, position and time. (Technically speaking, the
more accurate term for fields is to use the term <em>Lagrangian density</em>,
but we will use an abuse of terminology and simply call the Lagrangian
density &quot;the Lagrangian&quot;.)</p>
<p>The precise form of the Lagrangian depends highly on which exact field
we are studying. But if we want our theories to &quot;make sense&quot; in the
physical world, it must satisfy the following constraints:</p>
<ol>
<li>
<p>Any derivatives of the field that appear in the Lagrangian can be at
most <em>first derivatives</em>. That is, we can have
$\dfrac{\partial \phi}{\partial x}$ or $(\nabla \phi)^2$ as terms in
the Lagrangian, but <strong>not</strong> $\nabla^2 \phi$ as that would be
second-order. This is because we do not observe physical fields
satisfying partial differential equations (PDEs) that include
anything above a second-order derivative, and the process we will
soon see to determine the PDEs of the fields always raises the order
of the derivatives by one.</p>
</li>
<li>
<p>It must be a function of <em>individual positions</em> in space. Thus, the
Lagrangian cannot have a term that depends on two positions
$\vec x_1, \vec x_2$. This ensures that the field does not violate
<em>locality</em> (the fact that information propagates at a finite speed
and, therefore, information transfer cannot be instantaneous).</p>
</li>
<li>
<p>It must be scalar-valued and have units of <em>energy</em>, and we will see
why soon.</p>
</li>
</ol>
<p>There are further restrictions on the specific form of the Lagrangian of
a field based on the specific <strong>symmetries</strong> that we want the Lagrangian
(and thus our theory) to obey. For instance, if we want our theory to
have <em>translational symmetry</em> (symmetric in position), then the
Lagrangian <strong>cannot</strong> contain any terms that would lead to
$\mathscr{L}(\phi(\vec x + \vec \epsilon), \vec x + \vec \epsilon, t) \neq \mathscr{L}(\phi(\vec x, \vec t))$,
where $\vec \epsilon$ is some shift in position. We may impose further
restrictions by requiring, for instance, that the Lagrangian respects
<strong>rotational symmetry</strong> or symmetry upon &quot;flipping&quot; the field (which
is the transformation $\phi(\vec x, t) \to \phi(-\vec x, t)$). These
symmetries are desirable because a famous theorem known as <strong>Noether's
theorem</strong> says that certain symmetries lead to <em>conservation laws</em>, such
as the conservation of energy, momentum, and charge, which are
fundamental laws of nature. We will also encounter a specific type of
symmetry later on called <strong>Lorentz symmetry</strong> that imposes even stricter
conditions on the form the Lagrangian can take. In practice, the
combination of symmetries may often be so restrictive that there are
only a few possibilities for a field's Lagrangian that can satisfy all
the symmetries. But there is no absolute rule for finding the right
Lagrangian; even though we can often narrow down the Lagrangian into a
much simpler form, the specific Lagrangian that &quot;wins out&quot; in the end
is the one that leads to a theory that makes successful predictions,
verified by experiments. Physics, is ultimately an <em>empirical science</em>,
and the source of truth within theoretical physics, just like all other
branches of physics, is <em>consistency with observation</em>.</p>
<h3 id="the-principle-of-stationary-action">The Principle of Stationary Action</h3>
<p>Upon defining a Lagrangian, we can then write out a quantity known as
the <strong>action</strong>. The action $S$ is defined as:</p>
<p class="mathcell">
$$
S = \int \limits_\text{space+time} \mathscr{L}(\phi(\vec x, t), \vec x, t)\, d^4 x
$$
</p>
<p>Where $d^4 x = dx\, dy\, dz\, dt$, and the integral is conducted over
all space and all times. Why define this quantity? Because it turns out
that the correct partial differential equation to describe the field
$\phi(\vec x, t)$ is the one that makes the action <em>stationary</em>.
Stationary is a concept that comes from calculus; for the action, it
means that rate of change of the action with respect to change in $\phi$
must be zero, which we write as:</p>
<p class="mathcell">
$$
\delta S = 0
$$
</p>
<p>This requirement for the action being stationary is (unsurprisingly)
called the <strong>Principle of Stationary Action</strong>. It is also sometimes
called the Principle of <em>Least</em> Action, although the action may not
always be a minimum, so that alternate term can be misleading. It turns
out (we will not show here as it is a rather involved derivation) that
this means that the Lagrangian must satisfy the following tensor partial
differential equation:</p>
<p class="mathcell">
$$
\frac{\partial \mathscr{L}}{\partial \phi} - \partial_\mu \left(\frac{\partial \mathscr{L}}{\partial (\partial_\mu \phi)}\right) = 0
$$
</p>
<blockquote>
<p><strong>Note:</strong> It's okay to not understand what this equation means yet. We'll cover tensors in more detail shortly.</p>
</blockquote>
<p>This partial differential equation (PDE) is called the <strong>Euler-Lagrange
Equation</strong> for fields and always gives the correct PDE for the field. We
refer to this PDE as the <strong>equation of motion</strong> of the field, as it
describes how the field evolves through time and changes in space.
Solving the PDE (with suitable boundary conditions) allows us to find
the field as a function of position and time.</p>
<p>Rather incredibly, the basic mathematics and structure of classical
theories of fields carry over even to domains where classical mechanics
no longer holds - including the relativistic and quantum domains. This
is why it remains an important area of study over 200 years since its
inception.</p>
<h2 id="an-overview-of-tensors">An overview of tensors</h2>
<p>Tensors are some of the most elegant ways to write the laws of physics, used extensively in relativistic mechanics and relativistic quantum theory. However, they can be quite complicated to read and understand, so we will go over them here.</p>
<h3 id="what-is-a-tensor">What is a tensor?</h3>
<p>A tensor is a general name for a class of coordinate-independent objects. A scalar is a tensor, so is a vector, a matrix, and anything made of a combination of these. Whether we use spherical or cylindrical or cartesian coordinates, after all, the air temperature (a scalar field) doesn't change, the wind current velocity (a vector field) doesn't change, and the rotation of baseball hurtling towards me (a transformation matrix) also doesn't change. These are all examples of a general principle:</p>
<blockquote>
<p>The universe just doesn't care what coordinates we use to measure it. Coordinates are human constructs, not a fundamental fact of nature.</p>
</blockquote>
<p>However, when we impose a coordinate system to write out the equations of physics, we fix the values of the components. So a vector might be the same physical thing regardless of coordinate system, but its <em>components</em> are different in different coordinates. This can be an annoying issue: equations that might look simple in cartesian coordinates can grow monstrously annoying to read in, for instance, spherical coordinates. For example, this is Laplace's equation, often used for modelling gravity in a vacuum, in cartesian coordinates:</p>
<p class="mathcell">
$$
\nabla^2 f = 0
$$
</p>
<p>And this is its equivalent in spherical coordinates:</p>
<p class="mathcell">
$$
{\frac {1}{r^{2}}}{\frac {\partial }{\partial r}}\left(r^{2}{\frac {\partial f}{\partial r}}\right)+{\frac {1}{r^{2}\sin \theta }}{\frac {\partial }{\partial \theta }}\left(\sin \theta {\frac {\partial f}{\partial \theta }}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\frac {\partial ^{2}f}{\partial \varphi ^{2}}} = 0
$$
</p>
<p>With a change of coordinates, equations that looked elegant and easy to work with become clunky and untractable. What if there were a way to formulate physics in a way that doesn't depend on coordinates, and where we could use whichever coordinates we wished? This is where tensors come in. The tensor formulation of the same equation is given by:</p>
<p class="mathcell">
$$
\dfrac{1}{\sqrt{g}} \partial_i(\sqrt{g} g^{ij} \partial_j) f = 0, \quad g=\det(g_{ij})
$$
</p>
<p>How amazingly simple and graceful! <em>This</em> is why we use tensors.</p>
<h3 id="tensor-algebra-and-calculus">Tensor algebra and calculus</h3>
<p>To use tensors, it's helpful to start from our familiar vector and matrix formulas and build up from there.</p>
<p>For instance, consider a vector in Euclidean 3D space. Written in terms of a Cartesian basis, it is given by:</p>
<p class="mathcell">
$$
\vec V = V_x \hat e_x + V_y \hat e_y + V_z \hat e_z
$$
</p>
<p>It is common convention when writing tensors to write the components of vectors with superscript (upstairs) indices and their basis vectors with subscript (downstairs) indices. The reason will become apparent later, consider this just a convention for now. So we can rewrite as:</p>
<p class="mathcell">
$$
\vec V = V^x \hat e_x + V^y \hat e_y + V^z \hat e_z
$$
</p>
<p>If we set $x = 1, y = 2, z = 3$ we can equivalently write using <em>index</em> notation:</p>
<p class="mathcell">
$$
\vec V = V^1 \hat e_1 + V^2 \hat e_2 + V^3 \hat e_3 = \sum_{i = 1}^3 V^i \hat e_i
$$
</p>
<p>So a vector can be written in tensor form with:</p>
<p class="mathcell">
$$
\hat V = \sum_{i = 1}^3 V^i \hat e_i
$$
</p>
<p>When writing tensors, it is common practice to omit the summation sign; as long as the reader is aware that you are using tensors and that the summation is there even if it's not written out, the conveyed meaning remains the same, but the notation becomes much more compact:</p>
<p class="mathcell">
$$
\hat V = V^i \hat e_i
$$
</p>
<p>In a similar fashion, the dot product formula expressed using tensors can be written as:</p>
<p class="mathcell">
$$
S = A^i B_i
$$
</p><h3 id="upper-and-lower-indices">Upper and lower indices</h3>
<p>To be able to discuss tensors further, however, we must introduce two concepts: that of a <strong>co-vector</strong>, and of the <strong>metric</strong>. A co-vector is like a row vector in Cartesian coordinates. Remember that a row vector and a column vector create a scalar through their dot product:</p>
<p class="mathcell">
$$
\begin{bmatrix} a_1 &amp; b_1 &amp; c_1 \end{bmatrix}
\begin{bmatrix} a_2 \\ b_2 \\ c_2 \end{bmatrix} 
= a_1 a_2 + b_1 b_2 + c_1 c_2
$$
</p>
<p>A <strong>co-vector</strong> is the generalization of the idea of a row vector. We denote co-vectors with a subscript, such as $\vec V^T = V_i$ ($T$ here means &quot;transpose&quot; since a row vector is the transpose of a column vector). By contrast, we denote <strong>vectors</strong> with a superscript, such as $\vec W = W^j$.</p>
<p>For instance, consider the position vector $\vec x = x^i$, where the index $i$ indexes over the 3 coordinates of the position $(x^1, x^2, x^3) = (x, y, z)$. Because the position vector is a vector, and vectors are tensors, the position vector is <em>also</em> a tensor.</p>
<p>Now, the index of a tensor represents its <em>components</em>. This is important because two tensors with the same index have <em>different</em> components. For instance, we may have two tensors $\vec x = x^i$ and $\vec x' = x^j$, where the prime mark ($'$) indicates different coordinates. That is:</p>
<p class="mathcell">
$$
x^i = \begin{bmatrix} x \\ y \\ z \end{bmatrix}, \quad
x^j = \begin{bmatrix} x&#x27; \\ y&#x27; \\ z&#x27; \end{bmatrix}
$$
</p>
<p>This is important in tensor calculus, as it allows us to write tensors that have different components without needing to write a bunch of primed marks.</p>
<p>Just as we can for any vector, we can write the position <em>co-vector</em> $\vec x^T$ in tensor notation as $x_i$, and $\vec x^T$ as $x_j$. Therefore, we have:</p>
<p class="mathcell">
$$
x_i = \begin{bmatrix} x &amp; y &amp; z \end{bmatrix}, \quad
x_j = \begin{bmatrix} x&#x27; &amp; y&#x27; &amp; z&#x27; \end{bmatrix}
$$
</p>
<p>This gives us the ability to write out the dot product formula (which we just saw earlier) in tensor notation as:</p>
<p class="mathcell">
$$
\sum_i x_i x^i = x^2 + y^2 + z^2 = R^2
$$
</p>
<p>Where here, $R^2$ is the <strong>squared magnitude</strong> of the position vector, and is thus a <strong>scalar</strong>. Note that the expression $x_i x^j$ is <em>not</em> a valid expression for the dot product, because it would evaluate to:</p>
<p class="mathcell">
$$
\sum_i x_i x^i = xx&#x27; + y y&#x27; + zz&#x27; \neq x_i x^i
$$
</p>
<p>So it is important that we specify a tensor's index carefully so that <strong>tensors with different components have different indices</strong>, even if they are the same type of tensor. As we saw, a position vector $\vec x = x^i$ and a position vector with different components $\vec x' = x^j$ are both position vectors, but since they have <strong>different components</strong>, they must use <strong>different tensor indices</strong>.</p>
<h3 id="free-indices-dummy-indices-and-the-einstein-summation-convention">Free indices, dummy indices, and the Einstein summation convention</h3>
<p>After working with tensors for a while, it often becomes increasingly annoying to write out summation signs. For instance, recall our expression for the dot product:</p>
<p class="mathcell">
$$
\mathbf{A} \cdot \mathbf{B} = \sum_i A_i B^i
$$
</p>
<p>While this may not look so bad, consider taking the product of like-index components of two matrices. Then we would have:</p>
<p class="mathcell">
$$
AB = \sum_i \sum_j A_{ij} B^{ij}
$$
</p>
<p>But there are even higher-dimensional tensors, which means that we may encounter expressions in the form:</p>
<p class="mathcell">
$$
\sum_i \sum_j \sum_k A_{ijk} B^{ijk}
$$
</p>
<p>At this point, writing any more summation signs becomes a hassle. So instead, we often <em>drop</em> the summation signs, with the understanding that the summation signs are there, just not explicitly written-out. For instance, the dot product formula would just read $A_i B^i$, where we implicitly sum over $i$. This is called the <strong>Einstein summation convention</strong>.</p>
<p>The <em>Einstein summation convention</em> says that whenever an index appears once as a lower index and once as an upper index in a particular tensor expression, then the index is <strong>summed over</strong>. When this happens, we say that the twice-appearing index is a <strong>dummy index</strong> (also called a <strong>summation index</strong>), and we can choose to <em>freely change</em> the index's letter to any other letter. For instance, since the dot product in tensor notation has $i$ appearing twice - once as an upper index, and once as a lower index - we can change the index letter $i$ to any other letter, so the following are <strong>all equivalent</strong>:</p>
<p class="mathcell">
$$
x_i x^i = x_k x^k = x_r x^r = x_\delta x^\delta
$$
</p>
<p>A difference, however, is if we had an expression in the form $x_i x^j$. Since these are <strong>not</strong> a set of indentical upper and lower indices, they are <strong>not implicitly summed over</strong>. Thus, we say that they are <strong>free indices</strong>.</p>
<p>The key part of tensor algebra is that we must <em>keep track of our indices</em> such that all our equations have <em>balanced indices</em>. This means that the <strong>total number of free indices</strong> on both sides of an equation must be the <strong>same</strong>. Consider, for instance, the following expression:</p>
<p class="mathcell">
$$
R_{ij} = C_i B_j
$$
</p>
<p>Is this expression valid? Let's check the number of free indices. On the left, both $i, j$ are free indices, since neither appears twice (in an upper and lower index) in the expression. On the right, we also have two free indices, and they are the <em>same</em> free indices. So indeed, this tensor expression is valid. Note that if we changed this expression to:</p>
<p class="mathcell">
$$
R^i{}_j = C_i B^i
$$
</p>
<p>Then this expression would no longer be correct! On the left, we have the same two <em>free</em> indices $i, j$, but on the right, the index $i$ appears both as a lower and an upper index, so it is a <em>dummy index</em>, not a free index.</p>
<p>Let's examine a third case:</p>
<p class="mathcell">
$$
V_{ij} = q_i q^j e^k e_k
$$
</p>
<p>Would <em>this</em> expression be valid? If we check the left and right-hand sides, we find that both sides have two free indices - since $k$ appears as an upper and lower index in $e^k e_k$, it is a dummy index and doesn't count. But we now have a new problem: the index $j$ is a lower index on the left, and an <em>upper index</em> on the right! So this expression is <em>invalid</em>. But we could correct it by rewriting the expression with $j$ as a <em>lower index</em>:</p>
<p class="mathcell">
$$
V_{ij} = q_i q_j e^k e_k
$$
</p>
<p>Note that because $k$ is a dummy index, by the Einstein summation convention, we could change $k$ to any letter we want. Thus, it would be equally valid to write:</p>
<p class="mathcell">
$$
V_{ij} = q_i q_j e^\beta e_\beta
$$
</p>
<p>As a summary, tensors have two types of indices: <strong>free indices</strong> and <strong>dummy (summation) indices</strong>. Free indices <em>must</em> be balanced on the left- and right-hand sides of an equation, but dummy indices can be changed at will. This delicate balancing act - of making sure that all tensor equations have the same number of free indices - is the basis of tensor algebra, and as we'll see, tensor calculus. It makes sure that all our operations are mathematically valid, and lets us just concentrate on the physics without ever needing to think about the complex linear algebra that underlies all our operations.</p>
<h3 id="relabeling-indices-with-the-kronecker-delta">Relabeling indices with the Kronecker delta</h3>
<p>Previously, we've learned that indices are <em>fundamental</em> to tensors, and they are what distinguish tensors with different components. For instance, $x^i \neq x^j$, even though they are both position vectors, and the difference in index tells us that they have <em>different components</em>. Furthermore, since $i$ appears only one time in $x^i$, it is a <strong>free index</strong> - the same is true for $j$ in $x^j$. Thus, the tensor equation $x^i = x^j$ would automatically be invalid, since we'd have different free indices on the left-hand side and right-hand side.</p>
<p>However, it is sometimes convenient to <em>change</em> an index. Suppose that we wanted to turn $x^i$ <em>into</em> $x^j$. Think about it for a moment - how would we do that, using standard vectors and matrices? Well, a matrix maps one vector to another, so we would want some sort of &quot;conversion matrix&quot;, which (for reasons we'll see soon) we'll call $\delta_{ij}$:</p>
<p class="mathcell">
$$
x^i = \delta_{ij} x^j
$$
</p>
<p>Ah, but now our indices are unbalanced - both sides have $i$ as their free index ($j$ is a dummy index), but $i$ is a <em>lower index</em> on the right and an <em>upper index</em> on the left of the equation. So let's fix that by turning $i$ into an upper index on the right-hand side, too:</p>
<p class="mathcell">
$$
x^i = \delta^i {}_j x^j
$$
</p>
<p>And <em>there</em>, we've got it! This particular matrix $\delta^i{}_j$ is called the <strong>Kronecker delta</strong>, and it is also a tensor. It is defined as:</p>
<p class="mathcell">
$$
\delta_{ij} = \delta^{ij} = \delta^i{}_j = \delta_i{}^j = \begin{cases}
1, &amp; i = j \\
0, &amp; i \neq j
\end{cases}
$$
</p>
<p>Which also means that we can represent it as a sort of identity matrix:</p>
<p class="mathcell">
$$
\delta_{ij} = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \dots &amp; 1
\end{pmatrix}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Our preferred notation is to write the Kronecker delta with its upper and lower indices as $\delta^i{}_j$. But it is also common to just write $\delta^i_j$. <strong>Both forms are equivalent</strong>, it is only a difference of convention.</p>
</blockquote>
<p>The Kronecker delta's power to allow us to relabel indices is incredibly powerful. It means that we can turn a tensor with one set of indices, such as $A_{sl}$, into a tensor $A_{jk}$ with a completely different set of indices:</p>
<p class="mathcell">
$$
A_{jk} = \delta^s{}_j \delta^l{}_k A_{sl}
$$
</p>
<p>This expression, however, might be rather hard to read. A visual way of understaning it is to remember that we need to <em>balance</em> our  indices. When we apply the Kronecker delta, identical upper and the lower indices &quot;cancel&quot; out, so we can cross them out, like this:</p>
<p class="mathcell">
$$
A_{jk} = \delta^{\cancel{s}}{}_j \delta^{\cancel{l}}{}_k A_{\cancel{s}\cancel{l}}
$$
</p>
<p>Note how by &quot;crossing out&quot; the repeated $s$ and $l$ indices that both occur as an upper and lower index, we just get the indices $j, k$ left on the right-hand side, which then balances the left-hand side!</p>
<p>This is a powerful trick that we can use, for instance, to generalize the dot product by relabeling indices:</p>
<p class="mathcell">
$$
A^i B_j = A^i \delta^j{}_i B_j = A^i \delta^{\cancel{j}}{}_i B_{\cancel{j}} = A^i B_i
$$
</p>
<p>Here, by using the Kronecker delta, we could effectively &quot;cross out&quot; the index $j$, giving us just $A^i B_i$ left, which is just the dot product!</p>
<h3 id="raising-and-lowering-indices">Raising and lowering indices</h3>
<p>We saw how useful the Kronecker delta can be, when we wanted to relabel a tensor's index. But the Kronecker delta has a limitation: while it can <em>relabel</em> an index - for instance, changing $x^i$ to $x^j$ - it <em>can't</em> convert an upper index to a lower index.</p>
<p>To actually convert between lower and upper indices, we need to use the <strong>metric tensor</strong>. The metric tensor $g_{ij}$ is perhaps the <em>most</em> important tensor. It allows taking a tensor with a lower index, such as $x_j$, and converting it to a tensor with an upper index, like $x^i$:</p>
<p class="mathcell">
$$
x^i = g^{ij} x_j
$$
</p>
<p>This is called <strong>lowering an index</strong>. Likewise, the metric tensor also allows us to take a tensor with an upper index and converting it to a tensor with a lower index:</p>
<p class="mathcell">
$$
x_i = g_{ij} x^j
$$
</p>
<p>This is called <strong>raising an index</strong>. Furthermore, if we combine the Kronecker delta and the metric tensor, we can convert a tensor to its upper-index or lower-index equivalent with the <em>same</em> index:</p>
<p class="mathcell">
$$
\begin{align*}
x_i = \delta^j{}_i g_{ij} x^i = \delta^j{}_i x_j \\
x^i = \delta^i{}_j g^{ij} x_i =  \delta^i{}_j x^j
\end{align*}
$$
</p>
<p>Aside from just being useful, the metric tensor also has a very important place in physics. We will shortly discuss what the metric tensor $g_{ij}$ <em>physically represents</em>, but first, we need to cover a little bit more tensor algebra.</p>
<h3 id="tensor-contraction">Tensor contraction</h3>
<p>We saw previously that by the Einstein summation convention, identical upper and lower indices are summed over:</p>
<p class="mathcell">
$$
A_i B^i = A_1 B^1 + A_2 B^2 + A_3 B^3 + \dots + A_n B^n
$$
</p>
<blockquote>
<p><strong>Note:</strong> Remember that $A^1$ represents the <em>first component</em> of the vector $A_i$, and $A^2$ represents its <em>second component</em>. For instance, if we were working with Cartesian coordinates, $A^1 = A^x$ and $A^2 = A^y$. The indices $1, 2, 3,\dots$ are <em>tensor indices</em>, <strong>not exponents</strong>.</p>
</blockquote>
<p>But we can also do this for single tensors. For instance, suppose we have a tensor $T^i{}_j$. If we make the two indices identical, we would have $T^i{}_i$, which again is implicitly summed over $i$:</p>
<p class="mathcell">
$$
T^i{}_i = T^1{}_1 + T^2{}_2 + \dots + T^n{}_n
$$
</p>
<p>This is called a <strong>tensor contraction</strong> (also called the <strong>trace</strong>), and it's significant because it turns the tensor into a <em>scalar</em>. For instance, if our tensor $T_{ij}$ (we write it in the lower index for convenience) is given by:</p>
<p class="mathcell">
$$
T_{ij} = \begin{pmatrix}
T_{11} &amp; T_{12} &amp; T_{13} \\
T_{21} &amp; T_{22} &amp; T_{23} \\
T_{31} &amp; T_{32} &amp; T_{33}
\end{pmatrix}
$$
</p>
<p>Then the tensor contraction $T^i{}_i$ becomes:</p>
<p class="mathcell">
$$
T^i{}_i = T^1{}_1 + T^2{}_2 + T^3{}_3
$$
</p>
<p>Which is just the sum of the diagonals of $T_{ij}$ (with one raised index). We can also do this with tensors of three indices like $R^i{}_{jk}$, in which we have:</p>
<p class="mathcell">
$$
R^i{}_{ik} = R^1{}_{1k} + R^2{}_{2k} + R^3{}_{3k} + \dots + R^n{}_{nk}
$$
</p>
<p>The importance of the tensor contraction is that it takes a higher-dimensional tensor and returns a lower-dimensional tensor - often, returning a scalar. It is in many ways the <em>generalization</em> of the dot product (which takes two vectors and outputs a scalar) but for tensors of arbitrary dimensions. Being able to form a scalar from tensors is physically important because <em>scalars are invariant</em> - scalars are just numbers, and a number is a number is a number, no matter the coordinates we use. This makes it very important for <strong>relativistic theories</strong> - we're getting a bit ahead of ourselves here, but we'll get there soon!</p>
<h3 id="practicing-contractions">Practicing contractions</h3>
<p>Tensor contraction can be a bit unfamiliar, so let's do a practice problem: let's evaluate the expression $\delta^\mu {}_\mu \delta^\nu {}_\nu$, where $\mu, \nu = 0, 1, 2, 3$.</p>
<blockquote>
<p><strong>Note:</strong> Don't be scared by the weird choice of letters $\mu, \nu$ for indices! We can just as well use $i, j$, because dummy indices can use any letter we want.</p>
</blockquote>
<p>To solve, we expand the tensors with the implicit summations written out explicitly, This gives us:</p>
<p class="mathcell">
$$
\begin{align*}
\delta^\mu {}_\mu \delta^\nu {}_\nu &amp;= \underbrace{\sum_{\mu = 0}^3 \sum_{\nu = 0}^3 \delta^\mu {}_\mu \delta^\nu {}_\nu}_\text{write sums explicitly} \\
&amp;= \underbrace{\delta^0{}_0 \delta^0{}_0 + \delta^1{}_1 \delta^1{}_1 + \delta^2{}_2 \delta^2{}_2 + \delta^3{}_3 \delta^3{}_3}_\text{only nonzero terms in the sum are left} \\
&amp;= 1 + 1 + 1 + 1 \\
&amp;= 4
\end{align*}
$$
</p>
<p><strong>Why is this correct?</strong> This is due to the fact that <em>by definition</em> $\delta^i{}_i \delta^j{}_j$ is <em>only nonzero</em> if $i = j$. If we have $i = j$ then $\delta^i{}_i \delta^j{}_j = \delta^i{}_i \delta^i{}_i = 1$, but if we have $i \neq j$ we have $\delta_i^i \delta_j^j = 0$. Thus the summation (which would've usually been over 16 terms!) collapses only to four terms!</p>
<h3 id="tensor-calculus">Tensor calculus</h3>
<p>Just as we could do algebra with tensors, we can also do calculus with tensors. For instance, we could take the derivative of the position vector $x^i$ to get the velocity vector $v^i$:</p>
<p class="mathcell">
$$
v^i = \dfrac{dx^i}{dt}
$$
</p>
<p>We can also take <em>partial derivatives</em>. Suppose we had a scalar function $\phi(x^i) = \phi(x, y, z)$. Then, the partial derivative with respect to the potential can be written as:</p>
<p class="mathcell">
$$
\dfrac{\partial \phi}{\partial x^i} = \nabla \phi
$$
</p>
<p>In tensor notation, it is common to use the shorthand $\partial_i = \dfrac{\partial}{\partial x^i}$, meaning the above can also be written as:</p>
<p class="mathcell">
$$
\partial_i \phi = \nabla \phi
$$
</p>
<p>Remember that just like $\nabla \phi$ is a vector, $\partial_i \phi$ is also a vector! This means that $\partial_1 \phi = \partial_x \phi = \frac{\partial \phi}{\partial x}$, $\partial_2 \phi = \partial_y \phi = \frac{\partial \phi}{\partial y}$, and so on. The power of tensor calculus really starts, however, when we start differentiating <em>tensors</em> other than scalars. For instance, we can write the <strong>divergence</strong> as:</p>
<p class="mathcell">
$$
\partial_i F^i = \nabla \cdot \vec F
$$
</p>
<p>You may notice that since $i$ as both an upper and lower index, it is a dummy index: so this is a <em>tensor contraction</em>, which creates a scalar! Indeed, if we expand it, we have:</p>
<p class="mathcell">
$$
\partial_i F^i = \partial_x F^x + \partial_y F^y + \partial_z F^z
$$
</p>
<p>Which is just the standard vector calculus formula for the divergence. Additionally, just like any other tensor, we can raise and lower the index of the partial derivative $\partial_i$ using the metric. Thus, we have:</p>
<p class="mathcell">
$$
\partial^i = g^{ij} \partial_i
$$
</p>
<p>Treating the partial derivative as a tensor may be a bit uncomfortable, but it is mathematically made possible by the formalism of tensor calculus. It allows us to take otherwise very complicated derivatives, such as derivatives of a matrix:</p>
<p class="mathcell">
$$
\partial_i F^{ij} = J^j
$$
</p>
<p>To summarize, tensor calculus allows us to take derivatives of a variety of mathematical objects that go beyond just vectors and scalars. Crucially, it allows us to take complicated deriatives of tensors in ways that cannot be done with just vector calculus.</p>
<h3 id="relativity-and-spacetime">Relativity and spacetime</h3>
<p>We've covered a lot about tensors, but let's now return to a topic we discussed earlier: the <strong>metric tensor</strong>.</p>
<p>To begin, we have to discuss first the <em>geometric</em> origins of tensors. Tensors are not just arbitrary combinations of indices; they represent something with a particular set of <em>components</em>. These components are dependent upon the underlying space. For instance, a 2D vector $V^i$ represents a directional quantity, with two components $V^x, V^y$ in 2D Cartesian space. But that same vector could have <em>different</em> components if we use another coordinate system. Taking our example, if we switch to polar coordinates, the vector's components would switch to $V^r, V^\theta$. Thus, while tensors themselves are coordinate-independent - after all, the Universe doesn't care whether you use Cartesian or polar coordinates - the <em>components</em> of tensors are definitely dependent on the coordinate system we use, and the underlying geometry of space.</p>
<p>The <strong>metric tensor</strong> $g_{ij}$ lets us characterize a system of coordinates in a geometric space by defining what a <em>distance</em> represents. You might think - why do we need to formalize what a distance means? Isn't that <em>obvious</em>? But in fact, the notion of a distance is not a simple as it may seem. For instance, in Euclidean 3D space, the infinitesimal distance between two points is given by:</p>
<p class="mathcell">
$$
ds^2 = dx^2 + dy^2 + dz^2
$$
</p>
<p>&quot;I knew that!&quot;, you might say, &quot;that's just Pythagoras's theorem written with infinitesimals $dx, dy, dz$ rather than $x, y, z$!&quot; And indeed you would be correct. For hundreds of years, we have no reason to think that the Universe was anything other than 3D space with Euclidean geometry.</p>
<p>But at the dawn of the 20th century, the very famous Albert Einstein discovered that our Universe was not three-dimensional, as had been throught; rather, it was four-dimensional, containing one dimension of time and three of space - what we call <strong>spacetime</strong>. So in fact, the <em>correct</em> expression for the infinitesimal distance between two points is given by Pythagoras' formula:</p>
<p class="mathcell">
$$
ds^2 = dt^2 - (dx^2 + dy^2 + dz^2)
$$
</p>
<p>(For those readers already familiar with the Minkowski metric, be aware that we use units where $c$, the speed of light, is equal to one). Except this distance formula isn't completely correct either: Einstein found that this was a <em>special case</em> of a more general tensor expression for the distance between two points in spacetime:</p>
<p class="mathcell">
$$
\begin{align*}
ds^2 &amp;= g_{\mu \nu} dx^\mu dx^\nu \\
&amp;= g_{00} dt^2 + g_{01} dt\,dx + g_{02} dt\,dy + \dots \\
&amp;\qquad + g_{10} dx dt + g_{11} dx^2 + g_{12} dx\, dy + \dots \\
&amp;\qquad + g_{30} dz\,dt + g_{31} dz\,dx + g_{32} dz\,dy + g_{33}\,dz^2
\end{align*}
$$
</p>
<p>Where $g_{\mu \nu}$ is the four-dimensional <strong>metric tensor</strong>, given by: </p>
<p class="mathcell">
$$
g_{\mu \nu} = \begin{pmatrix}
g_{00} &amp; g_{01} &amp; g_{02} &amp; g_{03} \\
g_{10} &amp; g_{11} &amp; g_{12} &amp; g_{13} \\
g_{20} &amp; g_{21} &amp; g_{22} &amp; g_{23} \\
g_{30} &amp; g_{31} &amp; g_{32} &amp; g_{33}
\end{pmatrix}
$$
</p>
<p>Why is this significant? Because it means that distance is dependent on the <em>geometry</em> of spacetime, and that implies that time and space are interdependent, and that spacetime can be <strong>curved</strong>! In  fact, the metric tensor is the foundation of Einstein's theories of <strong>special and general relativity</strong>.</p>
<blockquote>
<p><strong>Note on notation:</strong> It is customary to use latin indices like $i, j, k, \dots$ when we are only talking about three-dimensional tensors, such as position $x^i$. However, when we are talking about <em>four-dimensional</em> tensors in spacetime, such as the metric tensor, we use greek indices like $\mu, \nu, \gamma$.</p>
</blockquote>
<p>In most of quantum field theory, we work in <strong>Minkowski spacetime</strong>, the <em>special case</em> of the four-dimensional spacetime that we discussed earlier. The correct expression for the infinitesimal distance in Minkowski space, as we saw, is given by:</p>
<p class="mathcell">
$$
ds^2 = dt^2 - (dx^2 + dy^2 + dz^2)
$$
</p>
<p>Note that we can also write this expression in matrix-vector form, as:</p>
<p class="mathcell">
$$
ds^2 = 
\begin{bmatrix} dt \\ dx \\ dy \\ dz \end{bmatrix}^T
\underbrace{\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1 \\
\end{pmatrix}}_{\eta_{\mu \nu}}
\begin{bmatrix} dt \\ dx \\ dy \\ dz \end{bmatrix}
$$
</p>
<p>By comparison with $ds^2 = g_{\mu \nu} dx^\mu dx^\nu$, we find that the matrix in the middle of the above expression is the <strong>metric tensor for Minkowski spacetime</strong>. We usually call this tensor the <em>Minkowski metric</em>, and denote it as $\eta_{\mu \nu}$:</p>
<p class="mathcell">
$$
\eta_{\mu \nu} = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1 \\
\end{pmatrix}
$$
</p>
<p>Quantum field theory is (mostly) based off Minkowski spacetime, so the Minkowski metric will be the metric tensor we use for raising and lowering indices. We'll wait to look into <em>general relativity</em> with its much more complicated curved spacetimes, but we'll get to that in time.</p>


        <!-- jump to TOC button -->
        
        <a id="jump-toc-btn" href="#toc">Show table of contents</a>
        <a href="../" class="return-link">Back to index</a>
        
    </article>

    <!-- We load KaTeX last for performance reasons -->
    <script defer src="https://www.learntheoreticalphysics.com/katex/katex.min.js"></script>
    <script defer src="https://www.learntheoreticalphysics.com/katex/contrib/auto-render.min.js"></script>
    <script defer src="https://www.learntheoreticalphysics.com/katex/contrib/mhchem.min.js"></script>
    <script defer src="https://www.learntheoreticalphysics.com/katex/contrib/copy-tex.min.js"></script>
    <script defer>
    function renderMath(element) {
        // renders math using KaTeX in a particular element
        renderMathInElement(element, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
          ],
          // • rendering keys, e.g.:
          throwOnError : false
        });
    }
    document.addEventListener("DOMContentLoaded", function() {
        var article = document.querySelector(".post");
        if (article) {
            renderMath(article);
        }
    })
    </script>
    
    <script defer>
    var toc = document.getElementById("toc");
    var tocPos = toc.offsetTop + toc.offsetHeight;
    var tocBtn = document.getElementById("jump-toc-btn");
        // return the top and bottom coordinates of the
        // user's position on the page
        function getViewport() {
            var scrollPosTop = document.documentElement.scrollTop;
            var scrollPosBottom = scrollPosTop + window.innerHeight;
            return scrollPosTop, scrollPosBottom
        }

    function showTocBtnOnScroll() {
            // don't show toc button on print media
            if (window.matchMedia('print').matches){ return; }
            var _, scrollBottom = getViewport();
            // get bottom of viewport scroll position
            if (scrollBottom > tocPos) {
                tocBtn.style.display = "block";
            } else {
                tocBtn.style.display = "none";
            }
        }

        window.onscroll = function(){
            showTocBtnOnScroll();
        };
    </script>
    


</body>

</html>
